{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_file):\n",
    "    \"\"\" This function reads the file in the location specified by string \n",
    "    `corpus_file` and returns a list of tuples (list of words in text, label)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    with open(corpus_file) as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            out.append((' '.join(tokens[3:]), tokens[1]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus('./Datasets/all_sentiment_shuffled.txt')\n",
    "print(\"Example:\\n\", \" Text: \", corpus[0][0], \"\\n  Label: \", corpus[0][1])\n",
    "print(\"Total number of documents =\", len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing requisite libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization vs Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using stemming intead of lemmatization due to the better results as below\n",
    "# This is just an example\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "tmp = [lemmatizer.lemmatize(w) for w in ['loving','stepped','running','ran']]\n",
    "tmp = [lemmatizer.lemmatize(w,pos='a') for w in ['loving','stepped','running','ran']]\n",
    "print(\"Output for Lemmatization : \" ,tmp)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "print(\"Output for Stemming : \",[ps.stem(word) for word in ['loving','stepped','running','ran']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING FUNCTIONS\n",
    "\n",
    "# Removes all the punctuations present in the document\n",
    "def remove_punctuation(doc):\n",
    "    # This removes all letters except range a-z and A-Z and replaces them with a space\n",
    "    proc_doc = re.sub('[^a-zA-Z]',' ',doc)\n",
    "    # Lowercasing the words\n",
    "    proc_doc = proc_doc.lower()\n",
    "    # Splitting the sentece into word list\n",
    "    proc_doc = proc_doc.split()\n",
    "    return proc_doc \n",
    "\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    # Removes words like 'if', 'he', 'she', 'the', etc which never belongs to any topic\n",
    "    proc_doc = [word for word in doc if not word in set(stopwords.words('english'))]\n",
    "    return proc_doc\n",
    "\n",
    "# lemmatizer is a transformers which transforms the word to its singular, present-tense form\n",
    "def lemmatize(doc):\n",
    "    # NOTE : Using Stemmer instead of Lemmatizer\n",
    "    ps = PorterStemmer()\n",
    "    proc_doc = [ps.stem(word) for word in doc]\n",
    "    return proc_doc \n",
    "    \n",
    "def preprocess(doc):\n",
    "    \"\"\" Function to preprocess a single document\n",
    "    \"\"\"\n",
    "    assert isinstance(doc, str)   # assert that input is a document and not the corpus\n",
    "    processed_doc = remove_punctuation(doc)\n",
    "    processed_doc = remove_stopwords(processed_doc)\n",
    "    processed_doc = lemmatize(processed_doc)\n",
    "    return processed_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING STEPS\n",
    "\n",
    "proc_doc = []; y = [] \n",
    "for i in range(len(corpus)):\n",
    "    proc_doc.append(preprocess(corpus[i][0]))\n",
    "    y.append(corpus[i][1]=='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Functions and Variables\n",
    "\n",
    "def list_str2list_word(list_str):\n",
    "    \"\"\" Function to convert list of strings to a list of words\n",
    "    \"\"\"\n",
    "    list_words = []\n",
    "    for string in list_str:\n",
    "        for word in string:\n",
    "            list_words.append(word)\n",
    "    return list_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes:\n",
    "    \"\"\" Class to implement the Naive-Bayes model on the given corpus\n",
    "    \n",
    "    Attributes:\n",
    "    \n",
    "    P_cj: List store the probablities of all classes in y_train\n",
    "    Pos_prob : List to store the conditional probablities of positive reviews\n",
    "    Neg_prob : List to store the conditional probablities of negative reviews\n",
    "    Vocab_Dict : Dictionary to store Vocabulary and corresponding indexes \n",
    "    \n",
    "    Functions:\n",
    "    train_nb : To train on the training documents and storing the probablities\n",
    "    \n",
    "    classify_nb :Classifies the test reviews using the conditional probablities \n",
    "                 and returns an array of predictions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self): \n",
    "        self.P_cj = []\n",
    "        self.Pos_prob = []\n",
    "        self.Neg_prob = []\n",
    "        self.Vocab_Dict = {}\n",
    "        \n",
    "    def train_nb(self,training_documents):\n",
    "        \n",
    "        # Converting the training documents to a bag of words\n",
    "        Tot_list = list_str2list_word(training_documents['Proc_Review'])\n",
    "        \n",
    "        # Learning Vocabulary and toring in Vocab_Dict\n",
    "        Vocabulary = list(set(Tot_list)); V = len(Vocabulary)\n",
    "        Vocabulary_Dict = {}\n",
    "        for i in range(V):\n",
    "            Vocabulary_Dict[Vocabulary[i]] = i\n",
    "        self.Vocab_Dict = Vocabulary_Dict\n",
    "        \n",
    "        # Making y_train binary(0,1) from Boolean Values\n",
    "        y_train = np.multiply(training_documents['Nature'],1)\n",
    "        \n",
    "        # Finding out probablities of all unique classes in y_train\n",
    "        P_cj = []\n",
    "        for cj in np.unique(y_train):\n",
    "            P_cj.append(np.unique(y_train,return_counts=True)[1][cj])\n",
    "        self.P_cj = P_cj/sum(P_cj)\n",
    "\n",
    "        # Storing the positive and negative reviews in two lists \n",
    "        Pos_list = []; Neg_list = []\n",
    "        for i in range(len(y_train)):\n",
    "            if(y_train[i]):  Pos_list = Pos_list+training_documents['Proc_Review'][i]\n",
    "            else: Neg_list = Neg_list+training_documents['Proc_Review'][i]\n",
    "\n",
    "        Pos_prob = np.zeros(V).tolist() \n",
    "        Neg_prob = np.zeros(V).tolist()\n",
    "\n",
    "        # COmputing the conditional probablities of words in both lists\n",
    "        for word in Pos_list:\n",
    "            Pos_prob[self.Vocab_Dict[word]] += 1\n",
    "        for word in Neg_list:\n",
    "            Neg_prob[self.Vocab_Dict[word]] += 1\n",
    "        \n",
    "        # Storing class attributes Po_prob and Neg_prob\n",
    "        self.Pos_prob = (Pos_prob+np.ones(V))/(sum(Pos_prob)+V)\n",
    "        self.Neg_prob = (Neg_prob+np.ones(V))/(sum(Neg_prob)+V)\n",
    "        pass\n",
    "\n",
    "    def classify_nb(self,test_documents):\n",
    "        # returns the guess of the classifier\n",
    "        y_pred = np.zeros(len(test_doc),dtype='int')\n",
    "        n_iter = 0\n",
    "        # Iterating through the test documents to find word matches \n",
    "        for string in test_documents:\n",
    "            P = np.log(self.P_cj[1]); N = np.log(self.P_cj[0])\n",
    "            for word in string:\n",
    "                if word in model.Vocab_Dict.keys():\n",
    "        # Computing total log probablities (since actual probablities are too small) \n",
    "                    P += np.log(self.Pos_prob[self.Vocab_Dict[word]])\n",
    "                    N += np.log(self.Neg_prob[self.Vocab_Dict[word]])\n",
    "        # Comparing log probablities and returing predictions\n",
    "            if(P<N): y_pred[n_iter] = 0\n",
    "            else: y_pred[n_iter] = 1\n",
    "            n_iter += 1\n",
    "        return y_pred    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Splitting and Predicting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Splitting the data into trainand test documents\n",
    "train_doc, test_doc, y_train, y_test = train_test_split(proc_doc, y, test_size = 0.20, random_state = 42)\n",
    "# Making the training documents into a dataset\n",
    "training_documents = pd.DataFrame(data = (train_doc,y_train)).transpose()\n",
    "training_documents.columns = ['Proc_Review','Nature']\n",
    "# Training the Naive Bayes model\n",
    "model = Naive_Bayes()\n",
    "model.train_nb(training_documents)\n",
    "# Making y_test binary(0,1) from Boolean Values\n",
    "y_test = np.multiply(y_test,1)\n",
    "# Finding model predictions\n",
    "y_pred = model.classify_nb(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def accuracy(y_test,y_pred,no_catgories=2):\n",
    "    \"\"\" Function to print the confusion matrix and accuracy given y_test and y_pred\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"The confusion matrix for the above classification problem :\\n\", cm)\n",
    "\n",
    "    #Computing accuracy\n",
    "    accuracy = sum(cm[i,i] for i in range(no_catgories))/(len(y_test))\n",
    "    print(\"The accuracy of the model is : \"+str(accuracy*100)+'%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the confusion matrix and accuracy for model\n",
    "accuracy(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
